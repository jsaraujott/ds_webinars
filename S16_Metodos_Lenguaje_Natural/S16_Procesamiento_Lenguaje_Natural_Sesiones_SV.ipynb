{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc40317",
   "metadata": {},
   "source": [
    "# Sprint 16 - Procesamiento de Lenguaje Natural (Sesiones)\n",
    "**Versión para estudiantes**\n",
    "\n",
    "Si bien la comunicación en los ámbitos del análisis y ciencia de datos se fundamenta generalmente en datos numéricos tabulares, la humanidad lo hace en base al lenguaje escrito, hablado o señalado. Esto genera una problemática en diversas circunstancias puesto que información relevante se transmite de forma poco estructurada y, en principio, incomprensible para nuestros propósitos analíticos. Lo anterior se dificulta aún más si se toma en cuenta que la debida comprensión de la comunicación humana requiere de una capacidad mental única en la naturaleza conocida, y que corresponde a la capacidad de conceptualizar y generar ideas para transmitirlas por medio del lenguaje.\n",
    "\n",
    "Para intentar solucionar en cierta medida esta problemática se ha desarrollado el campo de estudio del **procesamiento de lenguaje natural**, y en este caso vamos a aprender algunos aspectos relevantes sobre el mismo y cómo utilizarlo en nuestra etapa de ingeniería de atributos para incorporar variables extraídas de la comunicación humana en modelos predictivos. En concreto vamos a enfocarnos en tres métodos de procesamiento que se aplican actualmente:\n",
    "\n",
    "* Codificación por frecuencia.\n",
    "* Codificación por relevancia implícita.\n",
    "* Codificación por interpretación semántica.\n",
    "\n",
    "Complementario a lo anterior, en este caso buscamos aprender también técnicas que ayuden a procesar textos de forma efectiva, permitiéndonos simplificar su complejidad sin pérdida relevante de información.\n",
    "\n",
    "Finalmente, presentaremos un nuevo algoritmo a implementar en nuestro modelo predictivo, llamado **máquinas de vectores de soporte** o **SVM** por sus siglas en inglés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa3007",
   "metadata": {},
   "source": [
    "## Entendimiento del contexto\n",
    "\n",
    "Google ha percibido tus habilidades y potencial como científico de datos y te ha contratado para que trabajes en la unidad de negocio de Youtube. El primer proyecto que se te encarga consiste en crear un modelo capaz de identificar satisfactoriamente cuándo un comentario en videos virales es o no SPAM.\n",
    "\n",
    "Debes saber que la detección de SPAM es de suma importancia puesto que por un lado permite detectar cuentas maliciosas, fraudulentas o \"no-humanas\", lo cual mejora la experiencia de usuarios reales con la plataforma de videos más grande del mundo. Por otro, mejorar el posicionamiento de contenido popular en motores de búsqueda, permitiendo así una mejor interacción y una recomendación más asertiva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb04eb",
   "metadata": {},
   "source": [
    "## Entendimiento de los datos\n",
    "\n",
    "Para iniciar con tu proyecto, carga las librerías que vas a utilizar. Empieza por las librerías básicas con las que siempre trabajas para manipular y visualizar información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f88ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24a1dbc9",
   "metadata": {},
   "source": [
    "Carga además de **Scikit_Learn** lo siguiente:\n",
    "\n",
    "* Las funciones `train_test_split` y `clasiffication_report`.\n",
    "* La función `SVC` (Support Vector Classifier) del módulo `svm`.\n",
    "* Las funciones `CountVectorizer` y `TfidfVectorizer` del módulo `feature_extraction.text`, y que usaremos para codificar textos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02acbb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d83e44e",
   "metadata": {},
   "source": [
    "Carga finalmente las siguientes librerías, funciones y módulos que nos ayudarán a trabajar con textos:\n",
    "\n",
    "* `spacy` que nos permite cargar diccionarios de términos estandarizados (lemas) en distintos idiomas.\n",
    "* `re` que nos permite utilizar expresiones regulares para gestionar más fácilmente textos.\n",
    "* `torch` que nos permite trabajar con estructuras similares a matrices llamadas *tensores*.\n",
    "* `transformers` que nos permite implementar algoritmos de codificación avanzados.\n",
    "* `nltk` que permite identificar términos o palabras \"poco relevantes\".\n",
    "* La función `get_english_words_set` de la librería `english_words` que contiene diccionarios de palabras comúnmente aceptadas del idioma inglés.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a5d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fb2f2a6",
   "metadata": {},
   "source": [
    "Debido a las políticas de protección de datos personales que deben ser cumplidas cabalmente por empresas de este tipo, la variedad de información con la que puedes contar para cumplir con tu propósito está límitada solamente a todo aquello que sea por definición \"público\". Es decir, te será imposible emplear datos que permitan individualizar a los usuarios. Por esta razón, tienes a tu disposición el dataset **video_comments** que contiene la siguiente información de 1,711 comentarios realizados en cinco videos musicales que han sido virales en los últimos años:\n",
    "\n",
    "* comment_id: Número identificador único de cada comentario.\n",
    "* artist: Nombre del artista musical al que pertenece el video en el que se hizo el comentario.\n",
    "* song: Nombre de la canción expuesta en el video en el que se hizo el comentario.\n",
    "* date: Fecha en la que el comentario se publicó.\n",
    "* author: Nombre público del usuario que escribió el comentario.\n",
    "* content: Texto contenido en el comentario.\n",
    "* class: Tipo de mensaje de acuerdo a la clasificación realizada por Youtube (0: Comentario normal, 1: Spam)\n",
    "\n",
    "Explora el dataset a fin de establecer un objetivo técnico, el algoritmo y métricas a utilizar, y un plan de acción para preparación e ingeniería de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c080ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8fe173d",
   "metadata": {},
   "source": [
    "**OBJETIVO TÉCNICO**\n",
    "\n",
    "< Aquí tu respuesta >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300cf34b",
   "metadata": {},
   "source": [
    "**ALGORITMO Y MÉTRICAS DE RENDIMIENTO**\n",
    "\n",
    "< Aquí tu respuesta >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a56f707",
   "metadata": {},
   "source": [
    "**PLAN DE ACCIÓN PARA PREPARACIÓN E INGENIERÍA DE DATOS**\n",
    "\n",
    "< Aquí tu respuesta >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cde3f7",
   "metadata": {},
   "source": [
    "## Preparación de datos\n",
    "\n",
    "Lleva a cabo tu plan de acción para limpiar los textos de los comentarios. Al respecto, puntualicemos en algunas cosas evidentes que se observan en ellos y que deberían ser tratadas:\n",
    "* Caracteres y letras en mayúsuclas y minúsculas.\n",
    "* Códigos y entidades *html*.\n",
    "* Símbolos que que no son letras o caracteres, y que por tanto no que forman términos o frases conceptualmente comprensibles (emojis, caracteres especiales, etc.)\n",
    "\n",
    "Como punto de partida, extrae todos los textos y guárdalos en un nuevo dataset llamado `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904330bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca98b14",
   "metadata": {},
   "source": [
    "Crea una nueva columna llamada `content_clean` donde cambies todos los textos de la muestra para que estén en minúscula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d3e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d06e1f7",
   "metadata": {},
   "source": [
    "Sustituye en esta nueva columna los textos tipo código html de las formas `<\\w*\\s/>` y `<\\w*>` por un espacio. Utiliza estas expresiones regulares dentro de la función `re.sub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efa6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "071c1085",
   "metadata": {},
   "source": [
    "Cambia la entidad html dada por `\\&#39;` por la comilla simple '. Lo anterior debido a que en el idioma inglés esta comilla se utiliza como un método de abreviación propio del lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec028414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7f49002",
   "metadata": {},
   "source": [
    "Cambia las demás entidades html de la forma `\\&#*\\w*[0-9]*;*` por un espacio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044e0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02eef94d",
   "metadata": {},
   "source": [
    "Sustituye ahora todos los caracteres que no sean de tipo letra, espacios o la comilla de abreviación por un espacio. Esto es, cambia aquellos caracteres que sean de la forma `[^\\w\\s\\']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca1344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28d40f83",
   "metadata": {},
   "source": [
    "Dado que hemos quitado diversos caracteres a cambio de espacios, conviene además:\n",
    "\n",
    "* Eliminar los espacios sobrantes al inicio y al final de texto.\n",
    "* Eliminar los espacios entre palabras excesivos.\n",
    "\n",
    "Has estos ajustes en el orden indicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e1d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68fef211",
   "metadata": {},
   "source": [
    "En estos textos además existen palabras mal escritas, códigos extraños, jergas particulares o los muy conocidos \"errores de tipeo\". No nos interesan estos casos por lo que vamos a mantener solamente aquellos términos reconocidos usualmente en el idioma inglés. Utiliza el diccionario `web2` de la función **get_english_words_set** para limpiar los textos, y únelo con el diccionario `en_core_web_sm` que provee **spacy**. Este último debe instalarse previamente por la terminal con el comando siguiente:\n",
    "\n",
    "```ps\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f60365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c8e93ef",
   "metadata": {},
   "source": [
    "En una nueva columna llamada `content_lemma` estandariza finalmente los textos a través de una lematización. Este proceso consiste en eliminar conjugaciones de verbos, y flexiones o declinamentos de sustantivos, a fin de facilitar su procesamiento posterior. Puedes ayudarte para esto nuevamente con el diccionario `en_core_web_sm` de la librería **spacy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe0d4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cc87dcf",
   "metadata": {},
   "source": [
    "## Ingeniería de datos\n",
    "\n",
    "Ya tenemos nuestros textos suficientemente limpios como para proceder a codificarlos. Es importante recordar que más allá de la limpieza y lematización de un texto, se debe realizar un proceso de codificación sobre los mismos que permita a la computadora comprender lo que se encuentra allí escrito. Lo anterior ya que una máquina en principio no maneja conceptos de forma equivalente al cerebro humano, teniendo mayor facilidad para comprender y trabajar con números."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97787903",
   "metadata": {},
   "source": [
    "### Codificación por frecuencia\n",
    "\n",
    "Esta codificación parte de una lógica simple (pero no por eso menos conveniente) tal que cada texto de un corpus se divide en n-gramas o frases de una o más palabras. A partir de allí se hace un simple conteo de veces en que dichos términos se presentan. Tómese por ejemplo la frase \n",
    "\n",
    "*\"mi nombre es juan cual es tu nombre\"*\n",
    "\n",
    "La codificiación por frecuencia la transformaría en lo siguiente: \n",
    "\n",
    "```\n",
    "\"mi\":       1\n",
    "\"nombre\":   2\n",
    "\"es\":       2\n",
    "\"juan\":     1\n",
    "\"cual\":     1\n",
    "\"tu\":       1\n",
    "```\n",
    "\n",
    "De esta forma, se da una suerte de ponderación numérica a aquellos términos más comúnmente utilizados en el contexto; y este número puede servir como atributo para ser considerado por un algoritmo en un modelo predictivo.\n",
    "\n",
    "Vale señalar que resulta conveniente excluir de esta codificación a las llamadas \"palabras de para\", puesto que las mismas no aportan ninguna información más allá de ser nexos o puentes entre ideas relevantes. Volviendo a nuestro ejemplo, si quitamos las \"palabras de para\", la transformación quedaría así:\n",
    "\n",
    "```\n",
    "\"nombre\":   2\n",
    "\"juan\":     1\n",
    "\"cual\":     1\n",
    "```\n",
    "\n",
    "Notemos cómo la exclusión de estas palabras da mayor enfoque al contenido relevante del texto. \n",
    "\n",
    "Ahora bien, dado que en nuestro caso estamos trabajando con textos en idioma inglés, conviene descargar un diccionario que nos asista para esto. Haslo ejecutando el siguiente código:\n",
    "\n",
    "```py\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords_en = nltk.corpus.stopwords.words(\"english\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b09643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96e81cc7",
   "metadata": {},
   "source": [
    "Aplica entonces esta codificación en los textos lematizados. Utiliza la función `CountVectorizer` e incluye el argumento `stop_words` con las \"palabras de para\" extraídas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c03b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1ce0fa2",
   "metadata": {},
   "source": [
    "Visualiza los términos que presentan una mayor frecuencia en los textos estudiados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949395af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6827370",
   "metadata": {},
   "source": [
    "### Codificación por relevancia implícita\n",
    "\n",
    "También llamada codificación TF-IDF, este procedimiento va un paso más allá respecto al mostrado anteriormente, ya que adicional a calcular la frecuencia de un término específico (*TF: Text Frecuency*), se incorpora un factor adicional que \"castiga\" el continuo surgimiento de cada término en el conjunto de textos estudiados (*IDF: Inverse of Document Frecuency*). En este sentido, una palabra que aparece siempre en distintos textos tiene menor relevancia a nivel de cada caso específico, puesto que la información nueva que aporta es menor en el contexto de todo el documento.\n",
    "\n",
    "La forma de calcular esta relevancia $R_{n,t}$ para un término $t$, que se repite $f_{t,d}$ veces dentro de un texto $d$ que a su vez es parte de un conjunto o corpus $D$, se define a partir de la siguiente fórmula:\n",
    "\n",
    "$$ R_{t,d} = TF_{t,d} \\times IDF_{t,d} = \\frac{f_{t,d}}{\\sum_{i\\in d} f_{i,d}} \\times \\log \\left( \\frac{|D|}{|{d\\in D: t\\in d}|} \\right) $$\n",
    "\n",
    "Para entender mejor lo que aquí se busca, volvamos a nuestro ejemplo de la frase \"mi nombre es juan cual es tu nombre\", con su codificación por frecuencia dada por:\n",
    "\n",
    "```\n",
    "\"nombre\":   2   (0.50)\n",
    "\"juan\":     1   (0.25)\n",
    "\"cual\":     1   (0.25)\n",
    "```\n",
    "\n",
    "Entre paréntesis se ha incorporado frecuencia relativa (TF) de cada término. Consideremos ahora una respuesta dada por \n",
    "\n",
    "*\"hola mi nombre es ana\"* \n",
    "\n",
    "Esta frase tendría su propia codificación por frecuencia:\n",
    "\n",
    "```\n",
    "\"hola\":     1   (0.33)\n",
    "\"nombre\":   1   (0.33)\n",
    "\"ana\":      1   (0.33)\n",
    "```\n",
    "\n",
    "Es claro que ambos textos buscan comunicar los nombres de los emisores; pero además de esto, quisiéramos saber que características adicionales destacan en ambas frases. Calculemos por tanto el IDF de los términos del cada texto:\n",
    "\n",
    "```\n",
    "\"nombre\":   2   (0.50)  (0.00)\n",
    "\"juan\":     1   (0.25)  (0.30)\n",
    "\"cual\":     1   (0.25)  (0.30)\n",
    "```\n",
    "\n",
    "```\n",
    "\"hola\":     1   (0.33)  (0.30)\n",
    "\"nombre\":   1   (0.33)  (0.00)\n",
    "\"ana\":      1   (0.33)  (0.30)\n",
    "```\n",
    "\n",
    "Entonces, se tiene que la relevancia implícita de términos sería:\n",
    "\n",
    "```\n",
    "\"nombre\":   2   (0.50)  (0.00)  = 0.00\n",
    "\"juan\":     1   (0.25)  (0.30)  = 0.08\n",
    "\"cual\":     1   (0.25)  (0.30)  = 0.08\n",
    "```\n",
    "\n",
    "```\n",
    "\"hola\":     1   (0.33)  (0.30)  = 0.10\n",
    "\"nombre\":   1   (0.33)  (0.00)  = 0.00\n",
    "\"ana\":      1   (0.33)  (0.30)  = 0.10\n",
    "```\n",
    "\n",
    "Notemos que este indicador evidencia justamente estas características informativas adicionales que deseábamos conocer:\n",
    "\n",
    "* Aparte de que \"Juan\" dé su nombre, está preguntando \"cual\" es el nombre de la otra persona.\n",
    "* Aparte de responder con su nombre, \"Ana\" saluda a Juan con un \"hola\".\n",
    "\n",
    "Entendida su utilidad, codifica los textos lematizados de comentarios con la función `TfidfVectorizer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c29e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b76703ff",
   "metadata": {},
   "source": [
    "Visualiza los términos que presenten una mayor relevancia implícita en los textos estudiados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b3819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85f95037",
   "metadata": {},
   "source": [
    "### Codificación por interpretación semántica\n",
    "\n",
    "El desarrollo tecnológico de los últimos años ha permitido contar con diversos modelos capaces de procesar grandes volúmenes de datos a fin de traducir textos con una suerte de comprensión semántica del contenido de información. Estos modelos LLM (*Large Language Models*) han dado cabida al surgimiento de herramientas muy conocidas actualmente como Gemini (Google), ChatGPT/Copilot (OpenAI - Microsoft), Perplexity, DeepSeek, entre muchas otras; las cuales son capaces de interpretar el sentido y contenido de palabras, frases o documentos, prediciendo respuestas coherentes al contexto de las mismas.   \n",
    "\n",
    "De forma resumida, a continuación puedes visualizar el proceso que utilizan estos modelos:\n",
    "\n",
    "1. Reciben como entrada una palabra, frase o documento.\n",
    "2. Transforman esta entrada en términos conocidos como *tokens*. Para esto, el modelo cuenta con un diccionario propio predefinido que relaciona cada componente de la entrada con un identificador numérico único.\n",
    "3. Utilizan estos tokens como atributos de un algoritmo pre-entrenado. Por lo general este algoritmo es una red neuronal compleja llamada *transformador* que es capaz de vincular a través de pesos numéricos cada uno de los tokens recibidos. \n",
    "4. Este algoritmo genera como predicción un vector que contiene *marcadores semánticos* o *incrustaciones*. Estos resultados **codificados** actúan como una suerte de representación interna del significado percibido por el modelo respecto a esos tokens y al contexto general de la entrada.\n",
    "5. Las incrustaciones se utilizan como entradas para otro algoritmo pre-entrando que intenta decodificar estas señales en base a criterios probabilísticos que seleccionan nuevos tokens que mejor complementen la palabra, frase o documento recibido inicialmente.\n",
    "6. Cuando ya se tienen estos tokens de salida, se los transforma en términos humanos utilizando nuevamente el diccionario predefinido.\n",
    "\n",
    "Como puedes evidenciar, un modelo LLM cuenta básicamente con tres estructuras ya establecidas que se usan secuencialmente a lo largo del proceso:\n",
    "\n",
    "* Diccionario de tokens\n",
    "* Algoritmo codificador (transformador 1)\n",
    "* Algoritmo decodificador (transformador 2)\n",
    "\n",
    "Por lo que si te interesa adentrarte en el funcionamiento en detalle los transformadores, te recomiendo mirar el siguiente grupo de videos de Youtube: \n",
    "\n",
    "https://www.youtube.com/playlist?list=PLcCe-ymWq77ow42k4-ZrLzlM3F7Ha7smT\n",
    "\n",
    "Volviendo a nuestro caso en el que únicamente debemos llegar al punto 4 del proceso antes mencionado, el modelo que vamos a utilizar es el desarrollado por Google y que lleva el nombre de **BERT** (*Bidirectional Encoder Representations from Tranformers*). El transformador asociado a este modelo fue puesto en marcha en su primera versión el año 2018 y consiste en una red neuronal de 12 capas y más de 100 millones de hiperparámetros, que devuelven 768 marcadores semánticos para cualquier entrada que reciban.\n",
    "\n",
    "Para conocer su funcionamiento de mejor manera, empieza cargando el diccionario de tokens con el siguiente código:\n",
    "\n",
    "```py\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "df_diccbert = pd.DataFrame(dict(\n",
    "    termino = list(tokenizer.vocab.keys()),\n",
    "    token = list(tokenizer.vocab.values())\n",
    "))\n",
    "\n",
    "df_diccbert.sample(15)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8806b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5e30dd4",
   "metadata": {},
   "source": [
    "Crea una nueva columna en el corpus llamada `content_tokens`, en la cual se guarden a modo de vectores los tokens de cada uno de los textos limpios. Utiliza con este propósito el siguente código:\n",
    "\n",
    "```py\n",
    "def fun_bert(x):\n",
    "    n = 512\n",
    "    tokens = tokenizer.encode(x, add_special_tokens = True, max_length = n, truncation = True)\n",
    "    tokens = np.array(tokens[:n] + [0]*(n - len(tokens)))\n",
    "    return tokens\n",
    "\n",
    "corpus[\"content_tokens\"] = corpus[\"content_clean\"].apply(fun_bert)\n",
    "corpus.sample(15, random_state = 123)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bedfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a8bb30e",
   "metadata": {},
   "source": [
    "Algunas puntualizaciones relevantes asociadas a los resultados obtenidos:\n",
    "\n",
    "* El modelo BERT tradicional que utilizamos solamente es capaz de evaluar 512 tokens en cada texto y he allí la rezón detrás del valor impuesto en el argumento `max_length` dentro del código.\n",
    "* Como puedes evidenciar, todos los vectores de tokens comienzan con el identificador 101 (tambien llamado \"CLS - Classification\"). Este token es sin duda el más relevante pues es aquel que utilizará el transformador como agregador de la información contenida en cada uno de los textos.\n",
    "* Todos los vectores de tokens finalizan con 102 (tambien llamado \"SEP - Separator\"). Este token le especifica al trasformador donde concluye la información de cada texto. En efecto, luego de este identificador se colocan 0, los cuales representan que no existen datos adicionales a tener en cuenta.\n",
    "\n",
    "Respecto a este último punto, en este tipo de modelos conviene además crear una matriz buleana que le especifique al transformador qué parte de los textos son relevantes. Crea entonces lo que se conoce como **matriz de atención** aplicando el siguiente código:\n",
    "\n",
    "```py\n",
    "attn_mat = []\n",
    "for tokens in corpus[\"content_tokens\"]:\n",
    "    attn_mat.append(np.where(tokens != 0,1,0))\n",
    "\n",
    "attn_mat = np.array(attn_mat)\n",
    "print(attn_mat[:20,:15])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f7005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fbebeed",
   "metadata": {},
   "source": [
    "Carga el modelo BERT pre-entrenado de la librería **transformers** con el siguiente código:\n",
    "\n",
    "```py\n",
    "config = transformers.BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "mod_bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\", config = config)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116e51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2987cc41",
   "metadata": {},
   "source": [
    "Generemos ahora la codificación semántica deseada, aunque vamos con algo sencillo para empezar y comprender mejor los resultados. Extrae dos textos cualquiera entre todos los que tienes disponible, junto con las filas correspondientes de la matriz de atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5cbeb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0059e3b",
   "metadata": {},
   "source": [
    "Los transformadores requieren que los atributos a recibir sean de tipo **tensores**. Un tensor se puede entender como una generalización de las matrices que ya conoces, solamente ten en cuenta que mientras en una matriz no pueden darse más de dos dimensiones (filas y columnas), en un tensor pueden haber infinitas.\n",
    "\n",
    "![](mat_ten.png)\n",
    "\n",
    "Convierte los vectores de tokens de los dos textos seleccionados en **tensores** con la función `torch.tensor`. Has lo mismo con las atenciones extraídas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5495a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c67df20",
   "metadata": {},
   "source": [
    "Codifica los vectores de tokens con el modelo BERT aplicando el siguiente código:\n",
    "\n",
    "```py\n",
    "with torch.no_grad():\n",
    "    pred_bert_nn = mod_bert(\n",
    "        toks_tensor, \n",
    "        attention_mask = attn_tensor\n",
    "    )\n",
    "\n",
    "pred_bert = pred_bert_nn[\"pooler_output\"]\n",
    "pred_bert.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12903ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d9c70ab",
   "metadata": {},
   "source": [
    "Vale aclarar qué representan estas dos dimensiones obtenidas:\n",
    "\n",
    "* El valor de 2 representa el total de textos codificados.\n",
    "* El valor 768 corresponde a todas las incrustaciones o marcadores semánticos generados por este modelo.\n",
    "\n",
    "Visualiza estos resultados alcanzados a fin de que percibas la complejidad asociada a esta codificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ad2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bdbffd9",
   "metadata": {},
   "source": [
    "Resulta complejo interpretar estos valores. Pero para simplificar en cierta medida, podemos afirmar que si existen diferencias más marcadas entre las codificaciones de los textos, entonces su intepretación por parte del modelo será disinta a la hora de decodificar. En concreto, textos de comentarios que representan ideas similares tendrán codificaciones más parecidas frente a aquellas vistas entre textos conceptualmente distintos.\n",
    "\n",
    "Ahora bien, dado que ya sabemos cómo opera este codificador, aplícalo a todos los textos mediante el siguiente código que utiliza la librería **tqdm** para optimizar la velocidad de procesamiento (sí, los modelos LLM consumen muchos recursos computacionales por lo que la codificación podría tardar algunos minutos):\n",
    "\n",
    "```py\n",
    "# Cargar libreria tqdm para optimización de tiempo de codificación \n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tam_batch = 100\n",
    "incrustaciones = []\n",
    "\n",
    "for i in tqdm(range(len(corpus[\"content_tokens\"]) // tam_batch + 1)):\n",
    "\n",
    "    # Generar tensores de entrada\n",
    "    toks_tensor = torch.LongTensor(\n",
    "        corpus[\"content_tokens\"].iloc[tam_batch * i : tam_batch * (i + 1)].reset_index(drop = True)\n",
    "    )\n",
    "    attn_tensor = torch.LongTensor(\n",
    "        attn_mat[tam_batch * i : tam_batch * (i + 1)]\n",
    "    )\n",
    "\n",
    "    # Codificar textos\n",
    "    with torch.no_grad():\n",
    "        batch_inc_nn = mod_bert(\n",
    "            toks_tensor, \n",
    "            attention_mask = attn_tensor\n",
    "        )\n",
    "    \n",
    "    # Guardar resultados\n",
    "    incrustaciones.append(batch_inc_nn[\"pooler_output\"].numpy())\n",
    "\n",
    "# Consolidar resultados en dataframe\n",
    "X_codbert = pd.DataFrame(\n",
    "    np.concatenate(incrustaciones)\n",
    ")\n",
    "\n",
    "X_codbert.columns = [\"Inc_\" + str(x + 1) for x in range(X_codbert.shape[1])]\n",
    "X_codbert.sample(15, random_state = 123)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb839f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dc1bff5",
   "metadata": {},
   "source": [
    "### Particionamiento de datos\n",
    "\n",
    "Solamente faltan los últimos procesos de ingeniería. Define tu variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf9b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f61d33aa",
   "metadata": {},
   "source": [
    "Separa tus datos en conjuntos de entrenamiento y prueba. Recuarda que cuentas con 3 grupos de atributos a partir de las codificaciones realizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26e2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "090a6557",
   "metadata": {},
   "source": [
    "## Creación de modelo base\n",
    "\n",
    "El algoritmo **SVM** es un método de clasificación bastante popular debido principalmente a su eficiencia computacional y a su lógica consistente y robusta. La idea detrás de los mismos es que si un conjunto de datos es en principio \"separable\" en grupos o categorías, entonces es posible construir un hiperplano que los distinga, tal que la distancia de este con la observación más cercana de cada grupo (conocida como margen) se maximice. En las siguiente visualización se aclara de mejor manera este criterio para el caso de dos atributos.\n",
    "\n",
    "![](svm.png)\n",
    "\n",
    "Para efectos de su aplicación, el mencionado algoritmo tiene los siguienes hiperparámetros relevantes:\n",
    "\n",
    "* Gamma ($\\gamma$): nivel de sensibilidad del modelo. En otras palabras, que tan granular se desea que el hiperplano sea con los datos existentes. Se suele emplear de base $\\gamma = 1/k$, donde $k$ corresponde a la cantidad de atributos existentes.\n",
    "* C: factor de regularización. Es decir, grado de ajuste deseado en el proceso de maximización del margen al aplicar métodos de gradiente. Se suele emplear de base $C = 1$.\n",
    "\n",
    "Visto lo anterior, vale ejecutar el modelo con los distintos conjuntos creados y evaluar su rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cdf964",
   "metadata": {},
   "source": [
    "### Modelo SVM con codificación por frecuencia\n",
    "\n",
    "Crea, entrena y evalúa un modelo SVM usando como atributos los conteos obtenidos de la codificación por frecuencia. Utiliza la función `SVC` con los argumentos base para `gamma` y `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc673c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b94b5634",
   "metadata": {},
   "source": [
    "### Modelo SVM con codificación por relevancia\n",
    "\n",
    "Crea, entrena y evalúa un modelo SVM usando como atributos las relevancias obtenidas de la codificación TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3863f071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d01be9ad",
   "metadata": {},
   "source": [
    "### Modelo SVM con codificación BERT\n",
    "\n",
    "Crea, entrena y evalúa un modelo SVM usando como atributos las relevancias obtenidas de la codificación semántica (BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f57d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b284088",
   "metadata": {},
   "source": [
    "El modelo con atributos codificados con BERT se muestra significativamente mejor. Visualiza su asertividad con la matriz de confusión correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9acfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39ec6399",
   "metadata": {},
   "source": [
    "Has concluído satifactoriamente con tu primer proyecto en Youtube. Gracias a tu trabajo, la empresa cuenta con un modelo bastante asertivo para detectar si un comentario en videos es o no SPAM. Como parte del mejoramiento continuo de este modelo, podrías optimizar los hiperparámetros."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
